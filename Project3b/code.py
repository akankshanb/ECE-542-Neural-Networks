# -*- coding: utf-8 -*-
"""Final_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vh8V8PEttBpp1p7ubAFhrdsHZnMm1Byf
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from pandas import DataFrame
from keras.models import Sequential
from keras.layers import Convolution1D, MaxPooling1D
from keras.layers import Dense, Dropout, Flatten
from keras.layers import LSTM
from keras import preprocessing
from scipy import stats
from sklearn.model_selection import train_test_split
import pickle
from keras.callbacks import ModelCheckpoint
from google.colab import drive
from keras.layers import Activation, Conv1D, Reshape, TimeDistributed
from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/My Drive/Colab Notebooks/Training Data B/'

def dataextract(folder):
    dataset_arm = pd.read_csv(folder + "armIMU.txt", sep="  ", header=None, engine='python')
    dataset_wrist = pd.read_csv(folder + 'wristIMU.txt', sep="  ", header=None, engine='python')
    label = pd.read_csv(folder + 'detection.txt', sep="  ", header=None, engine='python')
    dataset_arm.columns = ["aa1", "aa2", "aa3", "ag1", "ag2", "ag3"]
    dataset_wrist.columns = ["wa1", "wa2", "wa3", "wg1", "wg2", "wg3"]
    label.columns = ["label"]
    dataset = pd.concat([dataset_arm, dataset_wrist], axis=1, sort=False)
    return dataset, label

def load_train_data(data,label):
    x = []
    for i in range(0, len(data), 40):
        p = data.loc[i:i + 150]
        values = p.values
        x.append(values)
    x = preprocessing.sequence.pad_sequences(x, maxlen=None, dtype='float64', padding='post', truncating='pre',
                                            value=0.0)
    data = np.array(x)
    y = []
    for i in range(0, len(label), 40):
        p = label.loc[i:i + 150]
        values = p.values
        mod = stats.mode(values)[0][0]
        y.append(mod)
    label = np.array(y)
    return data, label

def load_test_data(data,label):
    x = []
    for i in range(0, len(data)):
        p = data.loc[i:i + 150]
        values = p.values
        x.append(values)
    x = preprocessing.sequence.pad_sequences(x, maxlen=None, dtype='float64', padding='post', truncating='pre',
                                               value=0.0)
    data = np.array(x)
    y = []
    for i in range(0, len(label)):
        p = label.loc[i:i + 150]
        values = p.values
        mod = stats.mode(values)[0][0]
        y.append(mod)
    label = np.array(y)
    return data, label

print('loading data')

train_folder1 = path + 'Session01/'
train_folder2 = path + 'Session02/'
train_folder3 = path + 'Session03/'
train_folder4 = path + 'Session05/'
train_folder5 = path + 'Session06/'
train_folder6 = path + 'Session07/'
train_folder7 = path + 'Session12/'
train_folder8 = path + 'Session15/'
train_folder9 = path + 'Session16/'

test_folder1 =  path + 'Session13/'

Xtrain1, Ytrain1 = dataextract(train_folder1)
Xtrain2, Ytrain2 = dataextract(train_folder2)
Xtrain3, Ytrain3 = dataextract(train_folder3)
Xtrain4, Ytrain4 = dataextract(train_folder4)
Xtrain5, Ytrain5 = dataextract(train_folder5)
Xtrain6, Ytrain6 = dataextract(train_folder6)
Xtrain7, Ytrain7 = dataextract(train_folder7)
Xtrain8, Ytrain8 = dataextract(train_folder8)
Xtrain9, Ytrain9 = dataextract(train_folder9)
X_test, Y_test = dataextract(test_folder1)

X_train, Y_train = pd.concat([Xtrain1, Xtrain2, Xtrain3, Xtrain4, Xtrain5, Xtrain6, Xtrain7, Xtrain8, Xtrain9], ignore_index=True), pd.concat([Ytrain1, Ytrain2, Ytrain3, Ytrain4, Ytrain5, Ytrain6, Ytrain7, Ytrain8, Ytrain9], ignore_index=True)

X_train1, Y_train1= load_train_data(X_train, Y_train)
X_test1, Y_test1 = load_test_data(X_test, Y_test)

n_timesteps, n_features, n_outputs = 150, 12, 1
num_of_classes = 2
batch_size = 256
epochs = 10
learningRate = 1e-3

X_train1 = X_train1.reshape(-1,151,12,1)
Y_train1 = Y_train1.reshape(-1,1)
X_train1, X_val, Y_train1, Y_val1 = train_test_split(X_train1, Y_train1, test_size=0.30, random_state=2)
X_test1 = X_test1.reshape(-1,151,12,1)
Y_test1 = Y_test1.reshape(-1,1)

model = Sequential()
model.add(TimeDistributed(Conv1D(filters=32, kernel_size=2), input_shape =(151, 12, 1)))
model.add(TimeDistributed(Activation("relu")))
model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
model.add(TimeDistributed(Flatten()))
model.add(LSTM(500, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(200))
model.add(Dropout(0.2))
model.add(Dense(500, activation='relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(n_outputs, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())
print('\nBatch Size : ', batch_size,
      '\nEpochs : ', epochs,
      '\nSeq Length : ', n_timesteps)

hist_obj = model.fit(X_train1, Y_train1, validation_data=[X_val, Y_val1], epochs=epochs, batch_size=batch_size, verbose=1)

train_loss = hist_obj.history['loss']
val_loss = hist_obj.history['val_loss']
train_acc = hist_obj.history['acc']
val_acc = hist_obj.history['val_acc']

import matplotlib.pyplot as plt
plt.plot(train_acc)
plt.plot(val_acc)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.savefig(path + 'model_acc.png')
plt.show()
# "Loss"
plt.plot(train_loss)
plt.plot(val_loss)
plt.title('model loss')
plt.ylabel('loss')A
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.savefig(path + 'model_loss.png')
plt.show()

from keras.models import load_model
#model = model.save(path + 'my_model.h5')
model = load_model(path + 'my_model.h5')

_, accuracy = model.evaluate(X_test1, Y_test1, batch_size=batch_size, verbose=1)
print(accuracy)

test_folder2 = '/content/drive/My Drive/Colab Notebooks/Test Data 2/Session17/'
test_folder3 = '/content/drive/My Drive/Colab Notebooks/Test Data 2/Session11/'
test_folder4 = '/content/drive/My Drive/Colab Notebooks/Test Data 2/Session09/'
test_folder5 = '/content/drive/My Drive/Colab Notebooks/Test Data 2/Session08/'

def dataextract1(folder):    
    dataset_arm = pd.read_csv(folder + "armIMU.txt", sep="  ", header=None, engine='python')
    dataset_wrist = pd.read_csv(folder + 'wristIMU.txt', sep="  ", header=None, engine='python')
    dataset_arm.columns = ["aa1", "aa2", "aa3", "ag1", "ag2", "ag3"]
    dataset_wrist.columns = ["wa1", "wa2", "wa3", "wg1", "wg2", "wg3"]
    dataset = pd.concat([dataset_arm, dataset_wrist], axis=1, sort=False)
    return dataset
def load_test_data1(data):
    x = []
    for i in range(0, len(data)):
        p = data.loc[i:i + 150]
        values = p.values
        x.append(values)
    x = preprocessing.sequence.pad_sequences(x, maxlen=None, dtype='float64', padding='post', truncating='pre',
                                               value=0.0)
    data = np.array(x)
    return data

Xtest = dataextract1(test_folder1)

data = load_test_data1(Xtest)

data = data.reshape(-1,151,12,1)

ypred = model.predict_classes(data, batch_size=128, verbose=1)

from scipy import stats
x =[]
temp = ypred
for i in range(0, temp.shape[0]-150):
    temp[i+75][0] = stats.mode(temp[i:i+150])[0][0]
plt.plot(temp)

type(temp.tolist)

np.savetxt(path+ 'Predictions11.txt',ypred,fmt="%d")

test_pred = model.predict_classes(X_test1, batch_size=256, verbose=1)

plt.plot(temp)
plt.plot(Y_test)
plt.title('Prediction v/s Groundtruth')
plt.ylabel('Prediction values')
plt.xlabel('Groundtruth values')
plt.show()

from sklearn import metrics
metrics.f1_score(Y_test1,ypred)

X_test, Y_test = dataextract(test_folder1)

plt.plot(Y_test)

